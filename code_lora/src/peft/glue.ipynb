{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T08:22:52.934999Z",
     "start_time": "2024-12-05T08:22:50.129620Z"
    }
   },
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoModel\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:22:53.892779Z",
     "start_time": "2024-12-05T08:22:53.864229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "id": "64d3a824f7da2f99",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:22:55.172244Z",
     "start_time": "2024-12-05T08:22:55.168891Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7d7d4acd6706e969",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:24:32.837533Z",
     "start_time": "2024-12-05T08:24:30.012136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets\n"
   ],
   "id": "227afed244b6ad71",
   "outputs": [
    {
     "ename": "DataFilesNotFoundError",
     "evalue": "No (supported) data files found in glue",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mDataFilesNotFoundError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[1;32m----> 3\u001B[0m raw_datasets \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mglue\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmrpc\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m raw_datasets\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\load.py:2132\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2127\u001B[0m verification_mode \u001B[38;5;241m=\u001B[39m VerificationMode(\n\u001B[0;32m   2128\u001B[0m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS\n\u001B[0;32m   2129\u001B[0m )\n\u001B[0;32m   2131\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[1;32m-> 2132\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m load_dataset_builder(\n\u001B[0;32m   2133\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m   2134\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2135\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   2136\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   2137\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2138\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   2139\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   2140\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   2141\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   2142\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2143\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   2144\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   2145\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39mname \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2146\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[0;32m   2147\u001B[0m )\n\u001B[0;32m   2149\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[0;32m   2150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1853\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001B[0m\n\u001B[0;32m   1851\u001B[0m     download_config \u001B[38;5;241m=\u001B[39m download_config\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m download_config \u001B[38;5;28;01melse\u001B[39;00m DownloadConfig()\n\u001B[0;32m   1852\u001B[0m     download_config\u001B[38;5;241m.\u001B[39mstorage_options\u001B[38;5;241m.\u001B[39mupdate(storage_options)\n\u001B[1;32m-> 1853\u001B[0m dataset_module \u001B[38;5;241m=\u001B[39m dataset_module_factory(\n\u001B[0;32m   1854\u001B[0m     path,\n\u001B[0;32m   1855\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   1856\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   1857\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   1858\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   1859\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   1860\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   1861\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   1862\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39m_require_default_config_name,\n\u001B[0;32m   1863\u001B[0m     _require_custom_configs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbool\u001B[39m(config_kwargs),\n\u001B[0;32m   1864\u001B[0m )\n\u001B[0;32m   1865\u001B[0m \u001B[38;5;66;03m# Get dataset builder class from the processing script\u001B[39;00m\n\u001B[0;32m   1866\u001B[0m builder_kwargs \u001B[38;5;241m=\u001B[39m dataset_module\u001B[38;5;241m.\u001B[39mbuilder_kwargs\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1584\u001B[0m, in \u001B[0;36mdataset_module_factory\u001B[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001B[0m\n\u001B[0;32m   1575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m LocalDatasetModuleFactoryWithScript(\n\u001B[0;32m   1576\u001B[0m         combined_path,\n\u001B[0;32m   1577\u001B[0m         download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   1578\u001B[0m         dynamic_modules_path\u001B[38;5;241m=\u001B[39mdynamic_modules_path,\n\u001B[0;32m   1579\u001B[0m         trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   1580\u001B[0m     )\u001B[38;5;241m.\u001B[39mget_module()\n\u001B[0;32m   1581\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(path):\n\u001B[0;32m   1582\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m LocalDatasetModuleFactoryWithoutScript(\n\u001B[0;32m   1583\u001B[0m         path, data_dir\u001B[38;5;241m=\u001B[39mdata_dir, data_files\u001B[38;5;241m=\u001B[39mdata_files, download_mode\u001B[38;5;241m=\u001B[39mdownload_mode\n\u001B[1;32m-> 1584\u001B[0m     )\u001B[38;5;241m.\u001B[39mget_module()\n\u001B[0;32m   1585\u001B[0m \u001B[38;5;66;03m# Try remotely\u001B[39;00m\n\u001B[0;32m   1586\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_relative_path(path) \u001B[38;5;129;01mand\u001B[39;00m path\u001B[38;5;241m.\u001B[39mcount(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\load.py:840\u001B[0m, in \u001B[0;36mLocalDatasetModuleFactoryWithoutScript.get_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    834\u001B[0m     patterns \u001B[38;5;241m=\u001B[39m get_data_patterns(base_path)\n\u001B[0;32m    835\u001B[0m data_files \u001B[38;5;241m=\u001B[39m DataFilesDict\u001B[38;5;241m.\u001B[39mfrom_patterns(\n\u001B[0;32m    836\u001B[0m     patterns,\n\u001B[0;32m    837\u001B[0m     base_path\u001B[38;5;241m=\u001B[39mbase_path,\n\u001B[0;32m    838\u001B[0m     allowed_extensions\u001B[38;5;241m=\u001B[39mALL_ALLOWED_EXTENSIONS,\n\u001B[0;32m    839\u001B[0m )\n\u001B[1;32m--> 840\u001B[0m module_name, default_builder_kwargs \u001B[38;5;241m=\u001B[39m infer_module_for_data_files(\n\u001B[0;32m    841\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m    842\u001B[0m     path\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath,\n\u001B[0;32m    843\u001B[0m )\n\u001B[0;32m    844\u001B[0m data_files \u001B[38;5;241m=\u001B[39m data_files\u001B[38;5;241m.\u001B[39mfilter_extensions(_MODULE_TO_EXTENSIONS[module_name])\n\u001B[0;32m    845\u001B[0m \u001B[38;5;66;03m# Collect metadata files if the module supports them\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\load.py:601\u001B[0m, in \u001B[0;36minfer_module_for_data_files\u001B[1;34m(data_files, path, download_config)\u001B[0m\n\u001B[0;32m    599\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt infer the same data file format for all splits. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msplit_modules\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m module_name:\n\u001B[1;32m--> 601\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DataFilesNotFoundError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo (supported) data files found\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m (\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m    602\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module_name, default_builder_kwargs\n",
      "\u001B[1;31mDataFilesNotFoundError\u001B[0m: No (supported) data files found in glue"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:21:06.246455400Z",
     "start_time": "2024-11-28T03:06:38.502536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[100]\n"
   ],
   "id": "b190def4b86ed4d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'The Nasdaq composite index inched up 1.28 , or 0.1 percent , to 1,766.60 , following a weekly win of 3.7 percent .',\n",
       " 'sentence2': 'The technology-laced Nasdaq Composite Index .IXIC was off 24.44 points , or 1.39 percent , at 1,739.87 .',\n",
       " 'label': 0,\n",
       " 'idx': 114}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 178
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:21:06.246455400Z",
     "start_time": "2024-11-28T03:06:38.544748Z"
    }
   },
   "cell_type": "code",
   "source": "raw_train_dataset.features",
   "id": "d626310f0e38a27d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 179
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:21:07.589466Z",
     "start_time": "2024-12-05T08:21:07.023612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = \"FacebookAI/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "id": "32d9451300504270",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:21:43.217908Z",
     "start_time": "2024-12-05T08:21:43.207670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_func(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True,max_length=128)\n"
   ],
   "id": "2296f42603a6f51",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:21:44.234963Z",
     "start_time": "2024-12-05T08:21:44.221990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = raw_datasets.map(process_func, batched=True)\n",
    "tokenized_datasets"
   ],
   "id": "f1580999a7ebd6c8",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m tokenized_datasets \u001B[38;5;241m=\u001B[39m raw_datasets\u001B[38;5;241m.\u001B[39mmap(process_func, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      2\u001B[0m tokenized_datasets\n",
      "\u001B[1;31mNameError\u001B[0m: name 'raw_datasets' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:21:45.685619Z",
     "start_time": "2024-12-05T08:21:45.682053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# , remove_columns=raw_datasets.column_names\n",
    "# 分词后应删去不必要的列\n",
    "# 虽然对数据进行分词，但还需要对数据进行封装才可进入模型进行训练\n",
    "# 这里不能调用pytorch的DataLoader，需要使用transformers 的 DataCollatorWithPadding"
   ],
   "id": "869f2ad852228015",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:21:47.431685Z",
     "start_time": "2024-12-05T08:21:47.414705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n"
   ],
   "id": "33085223540bda2",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m tokenized_datasets \u001B[38;5;241m=\u001B[39m tokenized_datasets\u001B[38;5;241m.\u001B[39mremove_columns([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124midx\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m      2\u001B[0m tokenized_datasets \u001B[38;5;241m=\u001B[39m tokenized_datasets\u001B[38;5;241m.\u001B[39mrename_column(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m tokenized_datasets\u001B[38;5;241m.\u001B[39mset_format(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T03:06:39.255341Z",
     "start_time": "2024-11-28T03:06:39.252996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length=128)\n"
   ],
   "id": "4dc12f3fcfd42e14",
   "outputs": [],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T03:06:39.296104Z",
     "start_time": "2024-11-28T03:06:39.291859Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_datasets[\"train\"][0]",
   "id": "5be68efb6ec2171",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(1),\n",
       " 'input_ids': tensor([    0, 10127,  1001,  6182,  1238,    39,  2138,  2156,  2661,    37,\n",
       "           373,    22,     5,  4562,    22,  2156,     9, 12507,  7018, 23817,\n",
       "            39,  1283,   479,     2,     2, 48310,  4506,     7,   123,    25,\n",
       "           129,    22,     5,  4562,    22,  2156,  1918,  1001,  6182,  1238,\n",
       "            39,  2138,     9, 12507,  7018, 23817,    39,  1283,   479,     2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1])}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T03:06:39.333141Z",
     "start_time": "2024-11-28T03:06:39.330387Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_datasets[\"train\"].features",
   "id": "56cd79c8f8aeded6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 187
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T03:06:39.368905Z",
     "start_time": "2024-11-28T03:06:39.364307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]#取训练集前 8 列\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}#不需要这些列\n",
    "[len(x) for x in samples[\"input_ids\"]]#每一个样本的长度\n"
   ],
   "id": "5c2cf09351ad3d12",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 60, 48, 67, 60, 52, 62, 33]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T03:06:39.403961Z",
     "start_time": "2024-11-28T03:06:39.400593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}\n"
   ],
   "id": "3a65f60f1ac49173",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67])}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T03:06:39.440116Z",
     "start_time": "2024-11-28T03:06:39.437493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 如果是对序列进行分类就使用AutoModelForSequenceClassification；\n",
    "# num_labels=2是我们要改输出层，输出层不用预训练模型了，输出层自己训练。"
   ],
   "id": "9d1902d5c4494d6f",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:44:26.828334800Z",
     "start_time": "2024-11-28T08:37:18.886741Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
   "id": "12cae113fd5dab27",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 323
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:44:31.309198Z",
     "start_time": "2024-11-28T09:44:31.304236Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "4e1bb9cce4ad8ba2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ModulesToSaveWrapper(\n",
       "    (original_module): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "    (modules_to_save): ModuleDict(\n",
       "      (default): RobertaClassificationHead(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 376
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:44:33.523788Z",
     "start_time": "2024-11-28T09:44:33.519878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    print(name)"
   ],
   "id": "80ce024505975fbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.0.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "roberta.encoder.layer.0.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.0.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.1.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.1.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.2.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.2.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.3.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.3.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.4.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.4.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.5.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.5.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.6.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.6.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.6.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.6.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.6.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.6.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.7.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.7.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.7.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.7.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.7.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.7.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.8.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.8.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.8.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.8.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.8.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.8.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.9.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.9.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.9.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.9.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.9.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.9.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.10.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.10.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.10.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.10.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.10.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.10.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.11.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.11.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.11.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.11.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.11.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.11.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.original_module.dense.weight\n",
      "classifier.original_module.dense.bias\n",
      "classifier.original_module.out_proj.weight\n",
      "classifier.original_module.out_proj.bias\n",
      "classifier.modules_to_save.default.dense.weight\n",
      "classifier.modules_to_save.default.dense.bias\n",
      "classifier.modules_to_save.default.out_proj.weight\n",
      "classifier.modules_to_save.default.out_proj.bias\n"
     ]
    }
   ],
   "execution_count": 377
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# lora",
   "id": "3c164b689f223f43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:44:35.342852Z",
     "start_time": "2024-11-28T09:44:35.340855Z"
    }
   },
   "cell_type": "code",
   "source": "from peft import LoraConfig, TaskType, get_peft_model\n",
   "id": "cfe2134c0e058a47",
   "outputs": [],
   "execution_count": 378
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:44:36.065565Z",
     "start_time": "2024-11-28T09:44:36.061504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = LoraConfig(\n",
    "    task_type = TaskType.SEQ_CLS, \n",
    "    target_modules = [\"query\", \"value\"], \n",
    "    lora_alpha = 32,\n",
    "    r = 16,\n",
    "    bias = \"none\",\n",
    "    \n",
    "    )\n",
    "config\n",
    "# modules_to_save= [\"classifier.dense\", \"classifier.out_proj\"],"
   ],
   "id": "431a414d08d56772",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=<TaskType.SEQ_CLS: 'SEQ_CLS'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'query', 'value'}, exclude_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 379
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:44:37.027624Z",
     "start_time": "2024-11-28T09:44:36.993373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "lora_model.print_trainable_parameters()"
   ],
   "id": "b5f267c4d4a76bbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 125,534,212 || trainable%: 0.7066\n"
     ]
    }
   ],
   "execution_count": 380
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:44:37.546927Z",
     "start_time": "2024-11-28T09:44:37.542439Z"
    }
   },
   "cell_type": "code",
   "source": "lora_model",
   "id": "6eed74a684663005",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 381
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 训练",
   "id": "5b696cfb9ed3bdac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:44:38.863876Z",
     "start_time": "2024-11-28T09:44:38.860521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed = int(torch.randint(0, 2 ** 12 - 1, (1,)).item())\n",
    "seed"
   ],
   "id": "e7246695c673fe33",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37681"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 382
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:44:39.627229Z",
     "start_time": "2024-11-28T09:44:39.582010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./glue_mrpc\",\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    warmup_ratio = 0.1,\n",
    "    learning_rate = 2e-4,\n",
    "    optim = \"adamw_hf\",\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = seed,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "# \n",
    "training_args"
   ],
   "id": "df774b281af56760",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=IntervalStrategy.EPOCH,\n",
       "eval_use_gather_object=False,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=0.0004,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./glue_mrpc\\runs\\Nov28_17-44-39_hx,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.EPOCH,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=loss,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=30,\n",
       "optim=OptimizerNames.ADAMW_HF,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./glue_mrpc,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=16,\n",
       "per_device_train_batch_size=16,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./glue_mrpc,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=IntervalStrategy.EPOCH,\n",
       "save_total_limit=None,\n",
       "seed=37681,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.06,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       ")"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 383
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:44:40.356919Z",
     "start_time": "2024-11-28T09:44:40.353936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(pred):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "id": "48180ca8c1d64d66",
   "outputs": [],
   "execution_count": 384
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:44:40.914090Z",
     "start_time": "2024-11-28T09:44:40.900402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    lora_model,\n",
    "    training_args,\n",
    "    train_dataset = tokenized_datasets[\"train\"],\n",
    "    eval_dataset = tokenized_datasets[\"validation\"],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ],
   "id": "f5ca4ff140167816",
   "outputs": [],
   "execution_count": 385
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:49:27.884159Z",
     "start_time": "2024-11-28T09:44:41.871307Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()\n",
   "id": "27d1c145c5750535",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6900' max='6900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6900/6900 04:45, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.435071</td>\n",
       "      <td>0.877451</td>\n",
       "      <td>0.911661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.217800</td>\n",
       "      <td>0.322546</td>\n",
       "      <td>0.884804</td>\n",
       "      <td>0.916519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.387554</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.909735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.408915</td>\n",
       "      <td>0.877451</td>\n",
       "      <td>0.913194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0.369928</td>\n",
       "      <td>0.879902</td>\n",
       "      <td>0.913580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.329807</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.910369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.380385</td>\n",
       "      <td>0.870098</td>\n",
       "      <td>0.908463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.111700</td>\n",
       "      <td>0.489378</td>\n",
       "      <td>0.870098</td>\n",
       "      <td>0.909402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.515682</td>\n",
       "      <td>0.894608</td>\n",
       "      <td>0.925217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.529502</td>\n",
       "      <td>0.877451</td>\n",
       "      <td>0.912281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>0.589433</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.909408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.500937</td>\n",
       "      <td>0.887255</td>\n",
       "      <td>0.917266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.573378</td>\n",
       "      <td>0.877451</td>\n",
       "      <td>0.911972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.700148</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.911304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.570187</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.921708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.593013</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.595398</td>\n",
       "      <td>0.879902</td>\n",
       "      <td>0.912656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.588827</td>\n",
       "      <td>0.879902</td>\n",
       "      <td>0.914485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.569699</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.921986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.696042</td>\n",
       "      <td>0.889706</td>\n",
       "      <td>0.920071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>0.819412</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.783806</td>\n",
       "      <td>0.884804</td>\n",
       "      <td>0.917976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.787820</td>\n",
       "      <td>0.884804</td>\n",
       "      <td>0.915619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.843570</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.910053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>0.793528</td>\n",
       "      <td>0.879902</td>\n",
       "      <td>0.912966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.868316</td>\n",
       "      <td>0.877451</td>\n",
       "      <td>0.911348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.825628</td>\n",
       "      <td>0.879902</td>\n",
       "      <td>0.912343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.851661</td>\n",
       "      <td>0.879902</td>\n",
       "      <td>0.912966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.853199</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.907801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.859823</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.909414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\shaoc\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--glue\\05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Nov 27 15:46:17 2024) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6900, training_loss=0.08220917936684428, metrics={'train_runtime': 285.739, 'train_samples_per_second': 385.107, 'train_steps_per_second': 24.148, 'total_flos': 4370384095471392.0, 'train_loss': 0.08220917936684428, 'epoch': 30.0})"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 386
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T08:45:02.140265Z",
     "start_time": "2024-11-28T08:45:01.509771Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.save_model(output_dir=\"./glue_mrpc/lora\")",
   "id": "cd114a838cbdcf34",
   "outputs": [],
   "execution_count": 345
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 在测试集上进行测试",
   "id": "30a83817589fd844"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T08:42:34.864493Z",
     "start_time": "2024-11-28T08:42:28.882870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lora_model\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "# 为了进行比较需要对预测结果进行转换\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ],
   "id": "6d2e80d106c17adb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1725, 2) (1725,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8736231884057971, 'f1': 0.9031111111111111}"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 333
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# merge_model",
   "id": "9580b3c7a888f45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:00:14.200976Z",
     "start_time": "2024-11-28T09:00:11.931309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merge_model = lora_model.merge_and_unload()\n",
    "\n",
    "test_sentence1 = raw_datasets[\"test\"][\"sentence1\"]\n",
    "test_sentence2 = raw_datasets[\"test\"][\"sentence2\"]\n",
    "\n",
    "test_inputs = tokenizer(\n",
    "    test_sentence1, \n",
    "    test_sentence2, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "test_inputs.to(device)\n",
    "\n",
    "merge_model.eval()\n",
    "\n",
    "# 进行预测\n",
    "with torch.no_grad():\n",
    "    outputs = merge_model(**test_inputs)\n",
    "\n",
    "# 获取预测标签\n",
    "predictions = torch.argmax(outputs.logits, axis=-1)\n",
    "labels = tokenized_datasets[\"test\"][\"labels\"]\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions = predictions, references = labels)"
   ],
   "id": "820eaaae327723ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8736231884057971, 'f1': 0.9031111111111111}"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 366
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "969e38d66cda2d01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T08:47:34.167069Z",
     "start_time": "2024-11-28T08:47:33.800829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "merge_model = PeftModel.from_pretrained(base_model, \"./glue_mrpc/lora\")\n",
    "\n",
    "\n",
    "#base_model\n",
    "#print()\n",
    "merge_model\n"
   ],
   "id": "9929552c46cbf2cc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): RobertaClassificationHead(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 360
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T08:53:08.432304Z",
     "start_time": "2024-11-28T08:53:08.421892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_merge(original_weights, merged_weights):\n",
    "    if not torch.allclose(original_weights, merged_weights):\n",
    "        print(\"合并成功，权重发生了变化。\")\n",
    "    else:\n",
    "        print(\"权重没有变化，合并失败。\")\n",
    "\n",
    "is_merge(base_model.classifier.dense.weight, merge_model.classifier.dense.weight)"
   ],
   "id": "d0925fd3d187ec30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重没有变化，合并失败。\n"
     ]
    }
   ],
   "execution_count": 363
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T08:46:51.078340Z",
     "start_time": "2024-11-28T08:46:51.068912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_model.classifier.dense.weight = lora_model.classifier.dense.weight\n",
    "#base_model.classifier.dense.bias = lora_model.classifier.dense.bias\n",
    "base_model.classifier.out_proj.weight = lora_model.classifier.out_proj.weight\n",
    "#base_model.classifier.out_proj.bias = lora_model.classifier.out_proj.bias"
   ],
   "id": "839e5aec428dd78f",
   "outputs": [],
   "execution_count": 356
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T08:53:37.458045Z",
     "start_time": "2024-11-28T08:53:37.078290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merge_model = merge_model.merge_and_unload()\n",
    "merge_model.to(device)"
   ],
   "id": "b476ff76ccdbe009",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 364
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T08:53:52.598594Z",
     "start_time": "2024-11-28T08:53:52.588536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_merge(original_weights, merged_weights):\n",
    "    if not torch.allclose(original_weights, merged_weights):\n",
    "        print(\"合并成功，权重发生了变化。\")\n",
    "    else:\n",
    "        print(\"权重没有变化，合并失败。\")\n",
    "\n",
    "is_merge(base_model.classifier.dense.weight, merge_model.classifier.dense.weight)"
   ],
   "id": "a0426dfacc854b13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重没有变化，合并失败。\n"
     ]
    }
   ],
   "execution_count": 365
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:00:26.730257Z",
     "start_time": "2024-11-28T09:00:26.719314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# original_weights = lora_model.roberta.encoder.layer[0].attention.self.query.weight\n",
    "# merged_weights = merge_model.roberta.encoder.layer[0].attention.self.query.weight\n",
    "def is_merge(original_weights, merged_weights):\n",
    "    if not torch.allclose(original_weights, merged_weights):\n",
    "        print(\"合并成功，权重发生了变化。\")\n",
    "    else:\n",
    "        print(\"权重没有变化，合并失败。\")\n",
    "\n",
    "is_merge(lora_model.roberta.encoder.layer[0].attention.self.query.weight, merge_model.roberta.encoder.layer[0].attention.self.query.weight)"
   ],
   "id": "169e6ee69779d952",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重没有变化，合并失败。\n"
     ]
    }
   ],
   "execution_count": 367
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T08:47:01.177342Z",
     "start_time": "2024-11-28T08:46:55.749562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_sentence1 = raw_datasets[\"test\"][\"sentence1\"]\n",
    "test_sentence2 = raw_datasets[\"test\"][\"sentence2\"]\n",
    "\n",
    "test_inputs = tokenizer(\n",
    "    test_sentence1, \n",
    "    test_sentence2, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "test_inputs.to(device)\n",
    "\n",
    "merge_model.eval()\n",
    "\n",
    "# 进行预测\n",
    "with torch.no_grad():\n",
    "    outputs = merge_model(**test_inputs)\n",
    "\n",
    "# 获取预测标签\n",
    "predictions = torch.argmax(outputs.logits, axis=-1)\n",
    "labels = tokenized_datasets[\"test\"][\"labels\"]\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions = predictions, references = labels)"
   ],
   "id": "49b8ec6850c381e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8736231884057971, 'f1': 0.9031111111111111}"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 359
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T02:54:46.572845Z",
     "start_time": "2024-11-28T02:54:46.571032Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "35018cf875300f23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d72f941f120a5c74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T03:14:41.126560Z",
     "start_time": "2024-11-29T03:14:41.113554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ],
   "id": "614a6648ecc685a4",
   "outputs": [],
   "execution_count": 387
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1d39935d312f06e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "696e228450b6f0cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
